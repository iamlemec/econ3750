{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Differentiable Programming (with `jax`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import jax.lax as lax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure matplotlib output\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('config/clean.mplstyle') # this loads my personal plotting settings\n",
    "col = mpl.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have an HD display\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some warnings can get annoying\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `jax` Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of the `numpy` API is implemented in `jax`. Arrays have a device that they are stored on. If you have a supported GPU, it will default to this, otherwise it'll go on the CPU (or a TPU if you're really fancy, but I've never used one). Note that there is the notion of a **platform** which includes `cpu`, `gpu`, and `tpu`. Each platform has a certain number of devices (often just one though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GpuDevice(id=0, process_index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output here will depend on your setup (we'll be using this \"x\" a lot below too)\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "x.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n",
      "[GpuDevice(id=0, process_index=0)]\n"
     ]
    }
   ],
   "source": [
    "# you can print out available devices\n",
    "print(jax.devices('cpu'))\n",
    "print(jax.devices('gpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CpuDevice(id=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can send data between devices\n",
    "cpu0, *_ = jax.devices('cpu')\n",
    "xc = jax.device_put(x, cpu0)\n",
    "xc.device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays also have a `dtype`, which usually corresponds to those found in regular `numpy`. However, the defaults can be different and some are not supported on certain devices. The most common such difference is that `float64` is not supported unless you explicity enable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "float32\n",
      "int32\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# create some arrays\n",
    "print(np.arange(3).dtype)\n",
    "print(np.arange(3, dtype='float32').dtype)\n",
    "print(np.array([1, 2, 3]).dtype)\n",
    "print(np.array([1.0, 2, 3]).dtype)\n",
    "print(np.array([1, 2, 3], dtype='float32').dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 float32\n",
      "float32 float32\n"
     ]
    }
   ],
   "source": [
    "# should still be all float32\n",
    "print(x.dtype, x.astype('float64').dtype)\n",
    "print(xc.dtype, xc.astype('float64').dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can enable `float64` by running: `jax.config.update('jax_enable_x64', True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar Operations With `grad`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a scalar-to-scalar function derivatives. You can make a new function that returns the gradient by calling `grad` on a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a simple function (can be lambda too)\n",
    "def f(x):\n",
    "    return x**2\n",
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(6., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the gradient (derivative)\n",
    "df = jax.grad(f)\n",
    "df(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the second derivative\n",
    "d2f = jax.grad(df)\n",
    "d2f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got int32. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True.\n"
     ]
    }
   ],
   "source": [
    "# you need to make sure your inputs are floats\n",
    "try:\n",
    "    df(3)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we've only looked at functions of one parameter. Going to many parameters is fine, we just need to tell `grad` which variable to take the derivative with respect to. We do this by specifying the index of the desired parameter or parameters in the `argnums` flag. If you give it multiple indices, it will return a list of derivatives. The default is `argnums=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(58., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def g(x, y):\n",
    "    return y*x**2 + x*y**3\n",
    "dg = jax.grad(g, argnums=1)\n",
    "dg(2.0, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Operations With `grad` and `jacobian`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn to vector-to-scalar functions. Here, calling `grad` will give you a function that returns a vector with the same size as the input dimension. You don't have to specify the dimensions to `jax`, it'll figure these out the first time the function actually gets called. Because of this, if `jax` isn't happy about something, it may not give you an error until you actually try to use the output of `grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(20., dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second term is just to add some complexity\n",
    "def fv(x):\n",
    "    return np.sum(x**2) + x[1]*x[2]\n",
    "fv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 7., 8.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use the \"x\" defined above as a test input\n",
    "dfv = jax.grad(fv)\n",
    "dfv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[2., 0., 0.],\n",
       "             [0., 2., 1.],\n",
       "             [0., 1., 2.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the jacobian is non-diagonal because of the additional term\n",
    "jfv = jax.jacobian(dfv)\n",
    "jfv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient only defined for scalar-output functions. Output had shape: (3,).\n"
     ]
    }
   ],
   "source": [
    "# you can't use grad on vector-return functions! but maybe we can find another way...\n",
    "try:\n",
    "    dfe = jax.grad(lambda x: x**2)\n",
    "    dfe(x)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing With `vmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization is often necessary for performance in Python, especially in `jax`. Fortunately, we can do this easily with `vmap`. This function will take a input function and map it along a new dimension of your choice. For instance, you can use it to turn a scalar-to-scalar function into a vector-to-vector function. But there are many more possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1., 4., 9.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the most basic usage on the \"f\" defined above\n",
    "fv2 = jax.vmap(f)\n",
    "fv2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 4., 6.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can do the element-by-element gradient of a vector-return function\n",
    "dfv2 = jax.vmap(jax.grad(f))\n",
    "dfv2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just-In-Time Compilation With `jit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an approach to speeding up code by compiling it just when it is needed (rather than beforehand, like in C or C++). It's used in JavaScript, MATLAB, and other places. Calling `jit` on a function will return another function that does the same thing but faster. \n",
    "As with `grad`, it only actually does the compilation when you run the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 7., 8.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the vector gradient function above\n",
    "jdfv = jax.jit(dfv)\n",
    "jdfv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give it a much bigger vector for testing\n",
    "x2 = np.linspace(1.0, 5.0, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.6 ms ± 1.5 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# first do the un-jit-ed version\n",
    "%timeit -n 100 dfv(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 35.60 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "114 µs ± 229 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# now the jitted version (run twice for true comparison)\n",
    "%timeit -n 100 jdfv(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my computer this gives ~300x speed improvement. May vary from CPU to GPU though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees in `jax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are very cool! They're basically arbitrarily nested `dict`s and `list`s whose leaves can be any type. If you've seen JSON before, it's very similar to that. Why are trees cool? Just as `numpy` lets you perform operations over an entire array at once, trees let you do the same over entire trees as well. This let's you avoid a lot of clunky and error-prone packing and unpacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': DeviceArray([1., 2., 3., 4., 5.], dtype=float32),\n",
       " 'y': DeviceArray([10., 11., 12., 13., 14., 15., 16., 17., 18., 19.], dtype=float32)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first let's define a very simple tree, which is just a dict of arrays\n",
    "xd = {\n",
    "    'x': np.linspace(1.0, 5.0, 5),\n",
    "    'y': np.linspace(10.0, 19.0, 10),\n",
    "}\n",
    "xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a function that operates on such a tree\n",
    "# you could imagine specifying a model like this\n",
    "# where the input is a dict of parameters or data points\n",
    "def ft(d):\n",
    "    xsum = np.sum(d['x']**3)\n",
    "    ysum = np.sum(d['y']**2)\n",
    "    return xsum + ysum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': DeviceArray([ 3., 12., 27., 48., 75.], dtype=float32),\n",
       " 'y': DeviceArray([20., 22., 24., 26., 28., 30., 32., 34., 36., 38.], dtype=float32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can take a grad with respect to a tree and get a tree of the same shape back!\n",
    "dft = jax.grad(ft)\n",
    "dft(xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's easier to think in scalar terms them vectorize things. Suppose that our model really just operated on some `x` and `y` and we want to run the model for many such pairs. Then we can first define a slightly different function in purely scalar terms. I'm also adding in a separate parameter $\\alpha$ to make things more interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fts(d):\n",
    "    x, y = d['x'], d['y']\n",
    "    return x**3 + y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `vmap` to broadcast over the tree `d`. Below is what we'd write if we're interested in doing 10 distinct parameter sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 4.       ,  4.4581623,  4.949246 ,  5.481481 ,  6.0631013,\n",
       "              6.702332 ,  7.4074063,  8.186558 ,  9.04801  , 10.       ],            dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xd1 = {'x': np.linspace(0, 1, 10), 'y': np.linspace(2, 3, 10)}\n",
    "ftv1 = jax.vmap(fts)\n",
    "ftv1(xd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes we just wany to vary one parameter at a time. As with `grad`, we need to tell it which dimensions to operate on, this time using `in_axes`. But it's a bit more complicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([4.       , 4.001372 , 4.010974 , 4.037037 , 4.0877914,\n",
       "             4.171468 , 4.296296 , 4.4705076, 4.702332 , 5.       ],            dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xd2 = {'x': np.linspace(0, 1, 10), 'y': 2.0}\n",
    "ftv2 = jax.vmap(fts, in_axes=({'x': 0, 'y': None},))\n",
    "ftv2(xd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `in_axes` is a list of the same length as the number of function arguments (in this case 1). We then have to mirror the structure of the tree itself to specify that we want to map `x` over axis 0 and not map `y` at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops and Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... loops work a little differently in `jax`. It's okay to write a `for` loop actually, but not if you want to ultimately `jit` the thing. First, as we get into more advanced usage, there's a great overview of common \"gotchas\" in `jax` here: [JAX - The Sharp Bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very slow way to compute x**n\n",
    "def fl(x, n):\n",
    "    out = 1\n",
    "    for i in range(n):\n",
    "        out = x*out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The __index__() method was called on the JAX Tracer object Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerIntegerConversionError\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    jfl = jax.jit(fl)\n",
    "    jfl(2.0, 3)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a very helpful error message! But basically we tried to use a function argument as the loop bounds and then `jit` it. One way to take care of this is to specify that `n` is mostly fixed using `static_argnums`. This will work but will recompile for each distinct value of `n` that is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(8., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jfl = jax.jit(fl, static_argnums=1)\n",
    "jfl(2.0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way is to use one of the looping tools from `lax` such as `scan` or `fori_loop`. First, let's try it using `fori_loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(8., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fl_fori(x, n):\n",
    "    fl_x = lambda i, v: x*v\n",
    "    return lax.fori_loop(0, n, fl_x, 1.0)\n",
    "jfl_fori = jax.jit(fl_fori)\n",
    "jfl_fori(2.0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random numbers work a bit different in `jax` due to the different needs of Deep Learning folks. You need to generate a \"key\" first using a \"seed\" as input (below, `42` a very common seed). You then pass this key to various random number routines that are similar to those in `numpy.random`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 42]\n",
      "-0.18471177 -0.18471177\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "r1 = jax.random.normal(key)\n",
    "r2 = jax.random.normal(key)\n",
    "print(key)\n",
    "print(r1, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the same key leads to the same random numbers! So there must be more. To advance your key to something new you can use the `split` function, which is also deterministic. This will return two new keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2465931498 3679230171] [255383827 267815257]\n"
     ]
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "print(key, subkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one pattern is to keep a running `key` and generate a new `subkey` when you need another random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.2899989 , 0.82748747, 0.22911513, 0.2819779 , 0.8697449 ],            dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.random.uniform(subkey, (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
